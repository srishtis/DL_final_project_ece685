{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torchtext.datasets import BABI20\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.nn.init as I\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import re\n",
    "from functools import reduce \n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_question_story(data):\n",
    "    \"\"\"\n",
    "    This function concatenates the stories and the query separting them\n",
    "    by a special symbol Start of Query (SOQ)\n",
    "    Input: List of tuples of story, query, answer\n",
    "    Output: Merged list of story and answer\n",
    "    \"\"\"\n",
    "    merged_data = [(s+['SOQ']+q, [\"SOS\"]+a) for s,q,a in data]\n",
    "    return merged_data\n",
    "    \n",
    "# train_stories, test_stories = train_test_split(all_stories, test_size=0.2)\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "PAD_token = 2\n",
    "\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\"} # special symbols\n",
    "        self.n_words = 3  # Count SOS and EOS\n",
    "        self.word2index['SOS'] = 0\n",
    "        self.word2index['EOS'] = 1\n",
    "        self.word2index['PAD'] = 2\n",
    "        self.word2count['SOS'] = 1\n",
    "        self.word2count['EOS'] = 1\n",
    "        self.word2count['PAD'] = 1\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        # add sentence to the voacb\n",
    "        for word in sentence:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        # add words to vocab and create word to index and index to word\n",
    "        # dictionaries\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "def readLangs(all_stories):\n",
    "    pairs = merge_question_story(all_stories)\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    lang = Lang()\n",
    "    return lang, pairs\n",
    "\n",
    "def prepareData(all_stories):\n",
    "    lang, pairs = readLangs(all_stories)\n",
    "    for pair in pairs:\n",
    "        lang.addSentence(pair[0])\n",
    "        lang.addSentence(pair[1])\n",
    "    maxlen = 0\n",
    "    for pair in pairs:\n",
    "        sentence = pair[0]\n",
    "        if len(sentence)>maxlen:\n",
    "            maxlen = len(sentence)\n",
    "    return lang, pairs, maxlen\n",
    "\n",
    "def get_data_for_task(n):\n",
    "    \"\"\"\n",
    "    This function reads the file for given task, tokeizes, pads\n",
    "    and creates train and test dataloaders\n",
    "    Input: task number\n",
    "    Output: rain loder, test loader, lang class object\n",
    "    \"\"\"\n",
    "    with open(f'./tasks/task_{n}.txt') as f:\n",
    "        all_stories = get_stories(f, flatten=True)\n",
    "    lang, pairs ,maxlen = prepareData(all_stories)\n",
    "    train_pairs, test_pairs = train_test_split(pairs, test_size=0.2) \n",
    "    train_pairs = [tensorsFromPair(lang, pair, maxlen) for pair in train_pairs]\n",
    "    test_pairs = [tensorsFromPair(lang, pair, maxlen) for pair in test_pairs]\n",
    "    batch_size = 64\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_pairs, \n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_pairs, \n",
    "        batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader, lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence]\n",
    "\n",
    "def tensorFromSentence(lang, sentence, maxlen):\n",
    "    # this fucntion creates tensors from given sentence by tokenizing and adding\n",
    "    # special symbols\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    indexes = indexes + [PAD_token]*(maxlen-len(sentence))\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(lang, pair, maxlen):\n",
    "    input_tensor = tensorFromSentence(lang, pair[0], maxlen)\n",
    "    target_tensor = tensorFromSentence(lang, pair[1], 1)\n",
    "    return (input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the encoder class for the mdoel.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "\n",
    "        # Size of the one hot vectors that will be the input to the encoder\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Output size of the word embedding NN\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Number of layers in the lstm\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Regularization parameter\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.tag = True\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout = p)\n",
    "\n",
    " \n",
    "    def forward(self, x):\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding)\n",
    "\n",
    "        return hidden_state, cell_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the decoder class for the mdoel.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "\n",
    "        # Size of the one hot vectors that will be the input to the encoder\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Output size of the word embedding NN\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # Dimension of the NN's inside the lstm cell/ (hs,cs)'s dimension.\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        # Number of layers in the lstm\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Size of the one hot vectors that will be the output to the encoder (Vocab Size)\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # Regularization parameter\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.tag = True\n",
    "        self.embedding = nn.Embedding(self.input_size, self.embedding_size)\n",
    "        self.LSTM = nn.LSTM(self.embedding_size, hidden_size, num_layers, dropout = p)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "  \n",
    "    def forward(self, x, hidden_state, cell_state):\n",
    "        x = x.unsqueeze(0)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        outputs, (hidden_state, cell_state) = self.LSTM(embedding, (hidden_state, cell_state))\n",
    "        predictions = self.fc(outputs)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        return predictions, hidden_state, cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines sequence to sequence model. It creates the encoder and decoder. It \n",
    "    passes the input to the encoder, collects the ouput from encoder and passes it to the \n",
    "    decoder. The model uses teacher force to decide which input to use in the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, Encoder_LSTM, Decoder_LSTM):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.Encoder_LSTM = Encoder_LSTM\n",
    "        self.Decoder_LSTM = Decoder_LSTM\n",
    "\n",
    "    def forward(self, source, target, tfr=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = lang.n_words\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "        hidden_state_encoder, cell_state_encoder = self.Encoder_LSTM(source)\n",
    "\n",
    "        x = target[0] \n",
    "\n",
    "    for i in range(1, target_len-1):  \n",
    "        output, hidden_state_decoder, cell_state_decoder = self.Decoder_LSTM(x, hidden_state_encoder, cell_state_encoder)\n",
    "        outputs[i] = output\n",
    "        best_guess = output.argmax(1) # 0th dimension is batch size, 1st dimension is word embedding\n",
    "        x = target[i] if random.random() < tfr else best_guess # Either pass the next word correctly from the dataset or use the earlier predicted word\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss - 0.725\n",
      "\tTest aacuracy: 63.1%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tasks = [9]\n",
    "for task in tasks:\n",
    "    print(f\"Task {task}:\")\n",
    "    train_loader, test_loader, lang = get_data_for_task(task)\n",
    "    input_size_encoder = lang.n_words\n",
    "    encoder_embedding_size = 10\n",
    "    hidden_size = 10\n",
    "    num_layers = 1\n",
    "    encoder_dropout = float(0.3)\n",
    "\n",
    "    encoder_lstm = EncoderLSTM(input_size_encoder, encoder_embedding_size,\n",
    "                               hidden_size, num_layers, encoder_dropout).to(device)\n",
    "\n",
    "    input_size_decoder = lang.n_words\n",
    "    decoder_embedding_size = 10\n",
    "    hidden_size = 10\n",
    "    num_layers = 1\n",
    "    decoder_dropout = float(0.3)\n",
    "    output_size = lang.n_words\n",
    "\n",
    "    decoder_lstm = DecoderLSTM(input_size_decoder, decoder_embedding_size,\n",
    "                               hidden_size, num_layers, decoder_dropout, output_size).to(device)\n",
    "    \n",
    "    step = 0\n",
    "\n",
    "    model = Seq2Seq(encoder_lstm, decoder_lstm).to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    pad_idx = lang.word2index['PAD']\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "\n",
    "    epoch_loss = 0.0\n",
    "    num_epochs = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train(True)\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "\n",
    "            input = batch[0].to(device)\n",
    "            target = batch[1].to(device)\n",
    "\n",
    "            # Pass the input and target for model's forward method\n",
    "            output = model(input.squeeze().permute(1,0), target.squeeze().permute(1,0))\n",
    "            output = output[1:2].permute(1,0,2).reshape(-1, output.shape[2])\n",
    "            target = target.permute(1,0,2)\n",
    "            target = target[1:2].reshape(-1)\n",
    "\n",
    "            # Clear the accumulating gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Calculate the gradients for weights & biases using back-propagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the gradient value is it exceeds > 1\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "\n",
    "            # Update the weights values using the gradients we calculated using bp \n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    print(\"\\tLoss - {}\".format(np.round(loss.item(), 3)))\n",
    "    test_accuracy = []\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        input = batch[0].to(device)\n",
    "        target = batch[1].to(device)\n",
    "\n",
    "        output = model(input.squeeze().permute(1,0), target.squeeze().permute(1,0))\n",
    "        output_len = output.shape[0]\n",
    "        output = output[1:].reshape(output_len-1, -1, output.shape[2]).cpu().detach().numpy()\n",
    "        predicted_ans = np.argmax(output, axis=2)\n",
    "    #     print(output)\n",
    "    #     print(predicted_ans.shape)\n",
    "\n",
    "        target = target.permute(1,0,2)[1:].cpu().detach().numpy().squeeze()\n",
    "\n",
    "        test_accuracy += list(target[0,:]==predicted_ans[0,:])\n",
    "    print(f\"\\tTest aacuracy: {np.round(np.mean(np.hstack(test_accuracy))*100, 2)}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
