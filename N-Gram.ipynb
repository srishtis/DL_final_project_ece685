{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from functools import reduce \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file tasks already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = os.listdir('./tasks')\n",
    "text_files = [i for i in text_files if '.txt' in i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "task=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure of all tasks have single word answers\n",
    "for f in text_files:\n",
    "    text1 = pd.read_csv('./tasks/'+f, sep=\"\\n\", header=None)\n",
    "    text1.columns = ['text']\n",
    "    ans = []\n",
    "    for t in text1.text:\n",
    "        if '?' in t:\n",
    "            match = re.search(r'[a-zA-z0-9?\\ ]*\\t([\\w \\ ]+)', t)\n",
    "            if match:\n",
    "                ans.append(match.group(1)) \n",
    "                \n",
    "    ans = [i.split(' ') for i in ans]\n",
    "    for i in ans:\n",
    "        if len(i)>1:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [ x.strip() for x in re.split(r'(\\W+)', sent) if x.strip()]\n",
    "\n",
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A &amp;amp;amp;amp;amp; support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "def get_stories(f):\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines())\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    #flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    #data = [(flatten(story), q, answer) for story, q, answer in data]\n",
    "    data = [((story), q, answer) for story, q, answer in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_tensor(sequences,max_len):\n",
    "    \"\"\"\n",
    "    :param sequences: list of tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(sequences)\n",
    "    #max_len = max([len(s) for s in sequences])\n",
    "    out_dims = (num, max_len)\n",
    "    out_tensor = np.zeros((num, max_len))\n",
    "    for i, tensor in enumerate(sequences):\n",
    "        length = len(tensor)\n",
    "        out_tensor[i, :length] = tensor\n",
    "    return out_tensor\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    # story vector initialization\n",
    "    X = []\n",
    "    # query vector initialization\n",
    "    Xq = []\n",
    "    # answer vector intialization\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        # creating list of story word indices\n",
    "        x = [word_idx[w] for w in story]\n",
    "        # creating list of query word indices\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx))\n",
    "        # creating label 1 for the answer word index\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (padding_tensor(X,story_maxlen),\n",
    "            padding_tensor(Xq,query_maxlen), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 1 40.6%\n",
      "task 2 15.2%\n",
      "task 3 28.799999999999997%\n",
      "task 4 25.5%\n",
      "task 5 32.4%\n",
      "task 6 49.4%\n",
      "task 7 63.4%\n",
      "task 8 46.6%\n",
      "task 9 61.8%\n",
      "task 10 47.8%\n",
      "task 11 18.9%\n",
      "task 12 23.1%\n",
      "task 13 15.2%\n",
      "task 14 19.2%\n",
      "task 15 27.0%\n",
      "task 16 30.5%\n",
      "task 17 48.1875%\n",
      "task 18 47.480620155038764%\n",
      "task 19 9.5%\n",
      "task 20 94.95833333333333%\n"
     ]
    }
   ],
   "source": [
    "for f_i in range(len(task)):\n",
    "    file_path='./tasks/task_'+str(task[f_i])+\".txt\"\n",
    "    #print(file_path)\n",
    "    with open(file_path) as f:\n",
    "        all_stories = get_stories(f)\n",
    "    train_stories, test_stories = train_test_split(all_stories, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # creating the filtered qa sets\n",
    "    qa_sets_filtered_train=[]\n",
    "    for q_i in range(len(train_stories)):\n",
    "        # select only those sentences which have at least one word common with the question\n",
    "        results=[]\n",
    "        for i in range(len(train_stories[q_i][0])):\n",
    "\n",
    "            if(len(list(set(train_stories[q_i][0][i]) & set(train_stories[q_i][1])))>0):\n",
    "                results.append(train_stories[q_i][0][i])\n",
    "        qa_set_i= (results,train_stories[q_i][1],train_stories[q_i][2])\n",
    "        qa_sets_filtered_train.append(qa_set_i)\n",
    "        \n",
    "    #lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    #creating list of tuples for each story\n",
    "    data_filtered = [(flatten(story), q, answer) for story, q, answer in qa_sets_filtered_train]\n",
    "    \n",
    "    # creating the filtered qa sets\n",
    "    qa_sets_filtered_test=[]\n",
    "    for q_i in range(len(test_stories)):\n",
    "        # select only those sentences which have at least one word common with the question\n",
    "        results=[]\n",
    "        for i in range(len(test_stories[q_i][0])):\n",
    "\n",
    "            if(len(list(set(test_stories[q_i][0][i]) & set(test_stories[q_i][1])))>0):\n",
    "                results.append(test_stories[q_i][0][i])\n",
    "        qa_set_i= (results,test_stories[q_i][1],test_stories[q_i][2])\n",
    "        qa_sets_filtered_test.append(qa_set_i)\n",
    "        \n",
    "    #lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    #creating list of tuples for each story\n",
    "    data_filtered_test = [(flatten(story), q, answer) for story, q, answer in qa_sets_filtered_test]\n",
    "    \n",
    "    vocab = set()\n",
    "    for story, q, answer in (data_filtered + data_filtered_test):\n",
    "        vocab |= set(story + q + [answer])\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "    vocab_size = len(vocab) + 1\n",
    "    story_maxlen = max(map(len, (x for x, _, _ in data_filtered + data_filtered_test)))\n",
    "    query_maxlen = max(map(len, (x for _, x, _ in data_filtered + data_filtered_test)))\n",
    "    vocab = list(vocab)\n",
    "    vocab = ['<pad>'] + vocab\n",
    "    word_idx = dict((c, i) for i, c in enumerate(vocab))\n",
    "    idx_word = dict((i, c) for i,c in enumerate(vocab))\n",
    "\n",
    "    inputs_train, queries_train, answers_train = vectorize_stories(data_filtered,\n",
    "                                                                   word_idx,\n",
    "                                                                   story_maxlen,\n",
    "                                                                   query_maxlen)\n",
    "\n",
    "    inputs_test, queries_test, answers_test = vectorize_stories(data_filtered_test,\n",
    "                                                                word_idx,\n",
    "                                                                story_maxlen,\n",
    "                                                                query_maxlen)\n",
    "    \n",
    "    features_train=np.column_stack((queries_train,inputs_train))\n",
    "    target_train=np.where(answers_train == 1)[1].reshape(answers_train.shape[0],1)\n",
    "    \n",
    "    features_test=np.column_stack((queries_test,inputs_test))\n",
    "    target_test=np.where(answers_test == 1)[1].reshape(answers_test.shape[0],1)\n",
    "    \n",
    "    model = LinearSVC()\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions = model.predict(features_test)\n",
    "    #print(len(np.where(answers_test == 1)[1]))\n",
    "    print(\"task \"+str(task[f_i])+\" \"+str((accuracy_score(np.where(answers_test == 1)[1], predictions))*100)+\"%\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
