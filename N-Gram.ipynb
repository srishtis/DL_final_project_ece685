{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from functools import reduce \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file tasks already exists.\n"
     ]
    }
   ],
   "source": [
    "!mkdir tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = os.listdir('./tasks')\n",
    "text_files = [i for i in text_files if '.txt' in i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "task=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure of all tasks have single word answers\n",
    "for f in text_files:\n",
    "    text1 = pd.read_csv('./tasks/'+f, sep=\"\\n\", header=None)\n",
    "    text1.columns = ['text']\n",
    "    ans = []\n",
    "    for t in text1.text:\n",
    "        if '?' in t:\n",
    "            match = re.search(r'[a-zA-z0-9?\\ ]*\\t([\\w \\ ]+)', t)\n",
    "            if match:\n",
    "                ans.append(match.group(1)) \n",
    "                \n",
    "    ans = [i.split(' ') for i in ans]\n",
    "    for i in ans:\n",
    "        if len(i)>1:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [ x.strip() for x in re.split(r'(\\W+)', sent) if x.strip()]\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "    \n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        \n",
    "        #line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A &amp;amp;amp;amp;amp; support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "#             bigram_story = generate_ngrams(line, n)\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "def get_stories(f):\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines())\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    #flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    #data = [(flatten(story), q, answer) for story, q, answer in data]\n",
    "    data = [((story), q, answer) for story, q, answer in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_tensor(sequences,max_len):\n",
    "    \"\"\"\n",
    "    :param sequences: list of tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(sequences)\n",
    "    #max_len = max([len(s) for s in sequences])\n",
    "    out_dims = (num, max_len)\n",
    "    out_tensor = np.zeros((num, max_len))\n",
    "    for i, tensor in enumerate(sequences):\n",
    "        length = len(tensor)\n",
    "        out_tensor[i, :length] = tensor\n",
    "    return out_tensor\n",
    "\n",
    "\n",
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    # story vector initialization\n",
    "    X = []\n",
    "    # query vector initialization\n",
    "    Xq = []\n",
    "    # answer vector intialization\n",
    "    Y = []\n",
    "    for story, query, answer in data:\n",
    "        # creating list of story word indices\n",
    "        x = [word_idx[w] for w in story]\n",
    "        # creating list of query word indices\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx))\n",
    "        # creating label 1 for the answer word index\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "    return (padding_tensor(X,story_maxlen),\n",
    "            padding_tensor(Xq,query_maxlen), np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 1: 41.5%\n",
      "task 2: 28.7%\n",
      "task 3: 30.8%\n",
      "task 4: 20.5%\n",
      "task 5: 33.9%\n",
      "task 6: 52.3%\n",
      "task 7: 65.0%\n",
      "task 8: 43.5%\n",
      "task 9: 53.2%\n",
      "task 10: 69.9%\n",
      "task 11: 19.8%\n",
      "task 12: 27.3%\n",
      "task 13: 17.5%\n",
      "task 14: 18.7%\n",
      "task 15: 26.125%\n",
      "task 16: 25.0%\n",
      "task 17: 48.125%\n",
      "task 18: 56.008%\n",
      "task 19: 8.0%\n",
      "task 20: 84.417%\n"
     ]
    }
   ],
   "source": [
    "accuracies_unigram=[]\n",
    "for f_i in range(len(task)):\n",
    "    file_path='./tasks/task_'+str(task[f_i])+\".txt\"\n",
    "    #print(file_path)\n",
    "    with open(file_path) as f:\n",
    "        all_stories = get_stories(f)\n",
    "    train_stories, test_stories = train_test_split(all_stories, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # creating the filtered qa sets\n",
    "    qa_sets_filtered_train=[]\n",
    "    for q_i in range(len(train_stories)):\n",
    "        # select only those sentences which have at least one word common with the question\n",
    "        results=[]\n",
    "        for i in range(len(train_stories[q_i][0])):\n",
    "\n",
    "            if(len(list(set(train_stories[q_i][0][i]) & set(train_stories[q_i][1])))>0):\n",
    "                results.append(train_stories[q_i][0][i])\n",
    "        qa_set_i= (results,train_stories[q_i][1],train_stories[q_i][2])\n",
    "        qa_sets_filtered_train.append(qa_set_i)\n",
    "        \n",
    "    #lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    #creating list of tuples for each story\n",
    "    data_filtered = [(flatten(story), q, answer) for story, q, answer in qa_sets_filtered_train]\n",
    "    \n",
    "    # creating the filtered qa sets\n",
    "    qa_sets_filtered_test=[]\n",
    "    for q_i in range(len(test_stories)):\n",
    "        # select only those sentences which have at least one word common with the question\n",
    "        results=[]\n",
    "        for i in range(len(test_stories[q_i][0])):\n",
    "\n",
    "            if(len(list(set(test_stories[q_i][0][i]) & set(test_stories[q_i][1])))>0):\n",
    "                results.append(test_stories[q_i][0][i])\n",
    "        qa_set_i= (results,test_stories[q_i][1],test_stories[q_i][2])\n",
    "        qa_sets_filtered_test.append(qa_set_i)\n",
    "        \n",
    "    #lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    #creating list of tuples for each story\n",
    "    data_filtered_test = [(flatten(story), q, answer) for story, q, answer in qa_sets_filtered_test]\n",
    "    \n",
    "    vocab = set()\n",
    "    for story, q, answer in (data_filtered + data_filtered_test):\n",
    "        vocab |= set(story + q + [answer])\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "    vocab_size = len(vocab) + 1\n",
    "    story_maxlen = max(map(len, (x for x, _, _ in data_filtered + data_filtered_test)))\n",
    "    query_maxlen = max(map(len, (x for _, x, _ in data_filtered + data_filtered_test)))\n",
    "    vocab = list(vocab)\n",
    "    vocab = ['<pad>'] + vocab\n",
    "    word_idx = dict((c, i) for i, c in enumerate(vocab))\n",
    "    idx_word = dict((i, c) for i,c in enumerate(vocab))\n",
    "\n",
    "    inputs_train, queries_train, answers_train = vectorize_stories(data_filtered,\n",
    "                                                                   word_idx,\n",
    "                                                                   story_maxlen,\n",
    "                                                                   query_maxlen)\n",
    "\n",
    "    inputs_test, queries_test, answers_test = vectorize_stories(data_filtered_test,\n",
    "                                                                word_idx,\n",
    "                                                                story_maxlen,\n",
    "                                                                query_maxlen)\n",
    "    \n",
    "    features_train=np.column_stack((queries_train,inputs_train))\n",
    "    target_train=np.where(answers_train == 1)[1].reshape(answers_train.shape[0],1)\n",
    "    \n",
    "    features_test=np.column_stack((queries_test,inputs_test))\n",
    "    target_test=np.where(answers_test == 1)[1].reshape(answers_test.shape[0],1)\n",
    "    \n",
    "    model = LinearSVC(random_state=50)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions = model.predict(features_test)\n",
    "    #print(len(np.where(answers_test == 1)[1]))\n",
    "    accuracies_unigram.append(round((accuracy_score(np.where(answers_test == 1)[1], predictions))*100,3))\n",
    "    print(\"task \"+str(task[f_i])+\": \"+str(round((accuracy_score(np.where(answers_test == 1)[1], predictions))*100,3))+\"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    return [ x.strip() for x in re.split(r'(\\W+)', sent) if x.strip()]\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    #s = s.lower()\n",
    "    \n",
    "    # Replace all none alphanumeric characters with spaces\n",
    "    s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "    \n",
    "    # Break sentence in the token, remove empty tokens\n",
    "    tokens = [token.strip(' \\n ') for token in s.split(\" \") if token != \"\"]\n",
    "    # Use the zip function to help us generate n-grams\n",
    "    # Concatentate the tokens into ngrams and return\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "def parse_stories(lines,n):\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        \n",
    "        #line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A &amp;amp;amp;amp;amp; support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "            bigram_story = generate_ngrams(line, n)\n",
    "            #sent = tokenize(line)\n",
    "            story.append(bigram_story)\n",
    "    return data\n",
    "\n",
    "def get_stories(f,n):\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines(),n)\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    #flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    #data = [(flatten(story), q, answer) for story, q, answer in data]\n",
    "    data = [((story), q, answer) for story, q, answer in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 1 40.1%\n",
      "task 2 19.6%\n",
      "task 3 33.6%\n",
      "task 4 17.5%\n",
      "task 5 29.099999999999998%\n",
      "task 6 50.0%\n",
      "task 7 62.8%\n",
      "task 8 44.9%\n",
      "task 9 61.0%\n",
      "task 10 81.8%\n",
      "task 11 17.299999999999997%\n",
      "task 12 31.3%\n",
      "task 13 16.8%\n",
      "task 14 16.400000000000002%\n",
      "task 15 27.750000000000004%\n",
      "task 16 28.000000000000004%\n",
      "task 17 49.125%\n",
      "task 18 58.139534883720934%\n",
      "task 19 8.0%\n",
      "task 20 75.58333333333334%\n"
     ]
    }
   ],
   "source": [
    "accuracies_bigram=[]\n",
    "for f_i in range(len(task)):\n",
    "    file_path='./tasks/task_'+str(task[f_i])+\".txt\"\n",
    "    #print(file_path)\n",
    "    with open(file_path) as f:\n",
    "        all_stories = get_stories(f,n=2)\n",
    "    train_stories, test_stories = train_test_split(all_stories, test_size=0.2, random_state=42)\n",
    "\n",
    "    # creating the filtered qa sets\n",
    "    qa_sets_filtered_train=[]\n",
    "    for q_i in range(len(train_stories)):\n",
    "        # select only those sentences which have at least one word common with the question\n",
    "        results=[]\n",
    "        for i in range(len(train_stories[q_i][0])):\n",
    "            q_array=train_stories[q_i][1]\n",
    "            check=[]\n",
    "            for qsi in range(len(q_array)):\n",
    "                res = [k for k in train_stories[q_i][0][i] if q_array[qsi] in k]\n",
    "                check.append(len(res))\n",
    "            #print(check)\n",
    "            #print(sum(check))\n",
    "            if(sum(check)>0):\n",
    "                results.append(train_stories[q_i][0][i])\n",
    "        qa_set_i= (results,train_stories[q_i][1],train_stories[q_i][2])\n",
    "        qa_sets_filtered_train.append(qa_set_i)\n",
    "    \n",
    "    #print(qa_sets_filtered_train)\n",
    "    #lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    #creating list of tuples for each story\n",
    "    data_filtered = [(flatten(story), q, answer) for story, q, answer in qa_sets_filtered_train]\n",
    "        \n",
    "    # creating the filtered qa sets----test data\n",
    "    qa_sets_filtered_test=[]\n",
    "    for q_i in range(len(test_stories)):\n",
    "        # select only those sentences which have at least one word common with the question\n",
    "        results=[]\n",
    "        for i in range(len(test_stories[q_i][0])):\n",
    "            q_array=test_stories[q_i][1]\n",
    "            check=[]\n",
    "            for qsi in range(len(q_array)):\n",
    "                res = [k for k in test_stories[q_i][0][i] if q_array[qsi] in k]\n",
    "                check.append(len(res))\n",
    "            #print(check)\n",
    "            #print(sum(check))\n",
    "            if(sum(check)>0):\n",
    "                results.append(test_stories[q_i][0][i])\n",
    "        qa_set_i= (results,test_stories[q_i][1],test_stories[q_i][2])\n",
    "        qa_sets_filtered_test.append(qa_set_i)\n",
    "        \n",
    "    #lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    #creating list of tuples for each story\n",
    "    data_filtered_test = [(flatten(story), q, answer) for story, q, answer in qa_sets_filtered_test]\n",
    "    \n",
    "    vocab = set()\n",
    "    for story, q, answer in (data_filtered + data_filtered_test):\n",
    "        vocab |= set(story + q + [answer])\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "    vocab_size = len(vocab) + 1\n",
    "    story_maxlen = max(map(len, (x for x, _, _ in data_filtered + data_filtered_test)))\n",
    "    query_maxlen = max(map(len, (x for _, x, _ in data_filtered + data_filtered_test)))\n",
    "    vocab = list(vocab)\n",
    "    vocab = ['<pad>'] + vocab\n",
    "    word_idx = dict((c, i) for i, c in enumerate(vocab))\n",
    "    idx_word = dict((i, c) for i,c in enumerate(vocab))\n",
    "\n",
    "    inputs_train, queries_train, answers_train = vectorize_stories(data_filtered,\n",
    "                                                                   word_idx,\n",
    "                                                                   story_maxlen,\n",
    "                                                                   query_maxlen)\n",
    "\n",
    "    inputs_test, queries_test, answers_test = vectorize_stories(data_filtered_test,\n",
    "                                                                word_idx,\n",
    "                                                                story_maxlen,\n",
    "                                                                query_maxlen)\n",
    "    \n",
    "    features_train=np.column_stack((queries_train,inputs_train))\n",
    "    target_train=np.where(answers_train == 1)[1].reshape(answers_train.shape[0],1)\n",
    "    \n",
    "    features_test=np.column_stack((queries_test,inputs_test))\n",
    "    target_test=np.where(answers_test == 1)[1].reshape(answers_test.shape[0],1)\n",
    "    \n",
    "    model = LinearSVC(random_state=60, tol=1e-5)\n",
    "    model.fit(features_train, target_train)\n",
    "    predictions = model.predict(features_test)\n",
    "    #print(len(np.where(answers_test == 1)[1]))\n",
    "    accuracies_bigram.append(round((accuracy_score(np.where(answers_test == 1)[1], predictions))*100,3))\n",
    "    print(\"task \"+str(task[f_i])+\" \"+str((accuracy_score(np.where(answers_test == 1)[1], predictions))*100)+\"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_ngram_df = pd.DataFrame()\n",
    "resuts_ngram_df['Task']= [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "resuts_ngram_df['Unigram_accuracy'] = accuracies_unigram\n",
    "resuts_ngram_df['Bigram_accuracy'] = accuracies_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "resuts_ngram_df['Final']=np.where(resuts_ngram_df['Unigram_accuracy'] <= resuts_ngram_df['Bigram_accuracy'], resuts_ngram_df['Bigram_accuracy'], resuts_ngram_df['Unigram_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task</th>\n",
       "      <th>Unigram_accuracy</th>\n",
       "      <th>Bigram_accuracy</th>\n",
       "      <th>Final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>41.500</td>\n",
       "      <td>40.100</td>\n",
       "      <td>41.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>28.700</td>\n",
       "      <td>19.600</td>\n",
       "      <td>28.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>30.800</td>\n",
       "      <td>33.600</td>\n",
       "      <td>33.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>20.500</td>\n",
       "      <td>17.500</td>\n",
       "      <td>20.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>33.900</td>\n",
       "      <td>29.100</td>\n",
       "      <td>33.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>52.300</td>\n",
       "      <td>50.000</td>\n",
       "      <td>52.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>65.000</td>\n",
       "      <td>62.800</td>\n",
       "      <td>65.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>43.500</td>\n",
       "      <td>44.900</td>\n",
       "      <td>44.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>53.200</td>\n",
       "      <td>61.000</td>\n",
       "      <td>61.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>69.900</td>\n",
       "      <td>81.800</td>\n",
       "      <td>81.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>19.800</td>\n",
       "      <td>17.300</td>\n",
       "      <td>19.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>27.300</td>\n",
       "      <td>31.300</td>\n",
       "      <td>31.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>17.500</td>\n",
       "      <td>16.800</td>\n",
       "      <td>17.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>18.700</td>\n",
       "      <td>16.400</td>\n",
       "      <td>18.700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>26.125</td>\n",
       "      <td>27.750</td>\n",
       "      <td>27.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>25.000</td>\n",
       "      <td>28.000</td>\n",
       "      <td>28.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>48.125</td>\n",
       "      <td>49.125</td>\n",
       "      <td>49.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>56.008</td>\n",
       "      <td>58.140</td>\n",
       "      <td>58.140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>84.417</td>\n",
       "      <td>75.583</td>\n",
       "      <td>84.417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Task  Unigram_accuracy  Bigram_accuracy   Final\n",
       "0      1            41.500           40.100  41.500\n",
       "1      2            28.700           19.600  28.700\n",
       "2      3            30.800           33.600  33.600\n",
       "3      4            20.500           17.500  20.500\n",
       "4      5            33.900           29.100  33.900\n",
       "5      6            52.300           50.000  52.300\n",
       "6      7            65.000           62.800  65.000\n",
       "7      8            43.500           44.900  44.900\n",
       "8      9            53.200           61.000  61.000\n",
       "9     10            69.900           81.800  81.800\n",
       "10    11            19.800           17.300  19.800\n",
       "11    12            27.300           31.300  31.300\n",
       "12    13            17.500           16.800  17.500\n",
       "13    14            18.700           16.400  18.700\n",
       "14    15            26.125           27.750  27.750\n",
       "15    16            25.000           28.000  28.000\n",
       "16    17            48.125           49.125  49.125\n",
       "17    18            56.008           58.140  58.140\n",
       "18    19             8.000            8.000   8.000\n",
       "19    20            84.417           75.583  84.417"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resuts_ngram_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
