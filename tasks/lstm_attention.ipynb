{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from functools import reduce \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = os.listdir('./')\n",
    "text_files = [i for i in text_files if '.txt' in i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure of all tasks have single word answers\n",
    "for f in text_files:\n",
    "    text1 = pd.read_csv('./'+f, sep=\"\\n\", header=None)\n",
    "    text1.columns = ['text']\n",
    "    ans = []\n",
    "    for t in text1.text:\n",
    "        if '?' in t:\n",
    "            match = re.search(r'[a-zA-z0-9?\\ ]*\\t([\\w \\ ]+)', t)\n",
    "            if match:\n",
    "                ans.append(match.group(1)) \n",
    "                \n",
    "    ans = [i.split(' ') for i in ans]\n",
    "    for i in ans:\n",
    "        if len(i)>1:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(sent):\n",
    "    return tokenizer.tokenize(sent)\n",
    "\n",
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    nid_arr = []\n",
    "    for line in lines:\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "            nid_arr = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A &amp;amp;amp;amp;amp; support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            supporting = supporting.split()\n",
    "            supporting = [int(i) for i in supporting]\n",
    "            s = [i for i in range(len(nid_arr)) if nid_arr[i] in supporting]\n",
    "            data.append((substory, q, a, s))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "            nid_arr.append(nid)\n",
    "    return data\n",
    "\n",
    "def get_stories(f):\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines())\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    data = [((story), q, answer, s) for story, q, answer, s in data]\n",
    "    #data = [((story), q, answer) for story, q, answer in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['John', 'moved', 'to', 'the', 'garden'],\n",
       "  ['John', 'moved', 'to', 'the', 'bedroom'],\n",
       "  ['Mary', 'moved', 'to', 'the', 'garden'],\n",
       "  ['Daniel', 'went', 'to', 'the', 'office']],\n",
       " ['Where', 'is', 'Mary'],\n",
       " 'garden',\n",
       " [2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_query(sequences, query_maxlen):\n",
    "    \"\"\"\n",
    "    :param sequences: list of tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(sequences)\n",
    "    out_tensor = np.zeros((num, query_maxlen))\n",
    "    for i, tensor in enumerate(sequences):\n",
    "        length = len(tensor)\n",
    "        out_tensor[i, :length] = tensor\n",
    "    return out_tensor\n",
    "\n",
    "def padding_story(sequences, max_story_count, max_story_len):\n",
    "    \"\"\"\n",
    "    :param sequences: list of tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(sequences)\n",
    "    out_tensor = np.zeros((num, max_story_count, max_story_len))\n",
    "    for i, stories in enumerate(sequences):\n",
    "        for j, story in enumerate(stories):\n",
    "            length = len(story)\n",
    "            out_tensor[i, j, :length] = story\n",
    "    return out_tensor\n",
    "\n",
    "def padding_supporting(sequences, max_story_count):\n",
    "    \"\"\"\n",
    "    :param sequences: list of tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(sequences)\n",
    "    out_tensor = np.zeros((num, max_story_count))\n",
    "    for i, s in enumerate(sequences):\n",
    "        for j, t in enumerate(s):\n",
    "            if t>0:\n",
    "                out_tensor[i,t] = 1\n",
    "    return out_tensor\n",
    "\n",
    "def vectorize_stories(data, word_idx, query_maxlen, max_story_count, max_story_len):\n",
    "    # story vector initialization\n",
    "    X = []\n",
    "    # query vector initialization\n",
    "    Xq = []\n",
    "    # answer vector intialization\n",
    "    Y = []\n",
    "    S = []\n",
    "            \n",
    "    for story, query, answer, supporting in data:\n",
    "        # creating list of story word indices\n",
    "        x = []\n",
    "        for sen in story: \n",
    "            s = [word_idx[w] for w in sen]\n",
    "            x.append(s)\n",
    "        # creating list of query word indices\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx))\n",
    "        # creating label 1 for the answer word index\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        S.append(supporting)\n",
    "        \n",
    "    return (padding_story(X, max_story_count, max_story_len),\n",
    "            padding_query(Xq, query_maxlen), np.array(Y), padding_supporting(S, max_story_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(task):\n",
    "    with open(f'./task_{task}.txt') as f:\n",
    "        all_stories = get_stories(f)\n",
    "\n",
    "    train_stories, test_stories = train_test_split(all_stories, test_size=0.2)\n",
    "    max_story_count = max([len(story[0]) for story in all_stories])\n",
    "    max_story_len = max([max([len(i) for i in story[0]]) for story in all_stories])\n",
    "\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    vocab = set()\n",
    "    for story, q, answer, s in train_stories + test_stories:\n",
    "        vocab |= set(flatten(story) + q + [answer])\n",
    "\n",
    "    vocab = sorted(vocab)\n",
    "    vocab_size = len(vocab) + 1\n",
    "    #story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "    vocab = list(vocab)\n",
    "    query_maxlen = max(map(len, (x for _, x, _,_ in train_stories + test_stories)))\n",
    "    vocab = ['<pad>'] + vocab\n",
    "    word_idx = dict((c, i) for i, c in enumerate(vocab))\n",
    "    idx_word = dict((i, c) for i,c in enumerate(vocab))\n",
    "\n",
    "    train = vectorize_stories(train_stories,\n",
    "                               word_idx,\n",
    "                               query_maxlen, max_story_count, max_story_len)\n",
    "    test = vectorize_stories(test_stories,\n",
    "                               word_idx,\n",
    "                               query_maxlen, max_story_count, max_story_len)\n",
    "    return train, test, vocab, max_story_count, max_story_len, query_maxlen, word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_with_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, story_embed_size, query_embed_size, story_hidden_dim, query_hidden_dim, max_story_count, query_maxlen, max_story_len):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.story_embed_size = story_embed_size \n",
    "        self.story_hidden_dim = story_hidden_dim\n",
    "        self.query_hidden_dim = query_hidden_dim\n",
    "        self.story_embeddings = nn.Embedding(vocab_size, story_embed_size)\n",
    "        self.query_embeddings = nn.Embedding(vocab_size, query_embed_size)\n",
    "        self.story_lstm = nn.LSTM(story_embed_size*max_story_len, story_hidden_dim)\n",
    "        self.query_lstm = nn.LSTM(query_embed_size, query_hidden_dim)\n",
    "        self.story_hidden = self.init_story_hidden(max_story_count)  \n",
    "        self.query_hidden = self.init_query_hidden(query_maxlen) \n",
    "        self.hidden2label = nn.Linear(story_hidden_dim*max_story_count+query_hidden_dim*query_maxlen, vocab_size)\n",
    "        self.story_relevance = nn.Linear(story_hidden_dim*max_story_count, max_story_count)\n",
    "        \n",
    "    def forward(self, input_sequence, query, supported):\n",
    "        supporting_stories = input_sequence*supported.unsqueeze(2)\n",
    "        supporting_story_embed = self.story_embeddings(supporting_stories)\n",
    "        all_supporting_story_embed = self.story_embeddings(input_sequence)\n",
    "        query_embedding = self.query_embeddings(query)\n",
    "        supporting_story_embed = supporting_story_embed.view(supporting_story_embed.shape[0], supporting_story_embed.shape[1], -1)\n",
    "        all_supporting_story_embed = all_supporting_story_embed.view(all_supporting_story_embed.shape[0], all_supporting_story_embed.shape[1], -1)\n",
    "        lstm_story_out, _ = self.story_lstm(supporting_story_embed, self.story_hidden)\n",
    "        lstm_story_out_all, _ = self.story_lstm(all_supporting_story_embed, self.story_hidden)\n",
    "        lstm_query_out, _ = self.query_lstm(query_embedding, self.query_hidden)\n",
    "        s = lstm_story_out.view(len(lstm_story_out),-1)\n",
    "        s_all = lstm_story_out_all.view(len(lstm_story_out_all),-1)\n",
    "        q = lstm_query_out.view(len(lstm_query_out),-1)\n",
    "        \n",
    "        c = torch.cat([s,q], dim=1)\n",
    "        y  = self.hidden2label(c)\n",
    "        r = self.story_relevance(s_all)\n",
    "        log_probs = F.log_softmax(y)\n",
    "        log_probs_r = F.log_softmax(r)\n",
    "        return log_probs, log_probs_r\n",
    "        \n",
    "    \n",
    "        \n",
    "    def init_story_hidden(self, max_len):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, max_len, self.story_hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, max_len, self.story_hidden_dim)))\n",
    "    \n",
    "    def init_query_hidden(self, max_len):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, max_len, self.query_hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, max_len, self.query_hidden_dim)))\n",
    "\n",
    "def mutli_target_nll_loss(actual, predicted):\n",
    "    m = (actual*predicted).sum(axis=1)\n",
    "    loss = m.sum()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "mem = LSTM_with_attention(len(vocab), 10, 2, 5, 2)\n",
    "x=mem(torch.Tensor(inputs_train).long(), torch.Tensor(queries_train).long(), torch.Tensor(supporting_train).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "def train_model(model, optimizer, epochs, story, query, supporting, answers, test):\n",
    "    pad = word_idx['<pad>']\n",
    "    test_accuracy = []\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        outputs, relevance = model(story, query, supporting)\n",
    "        loss = F.nll_loss(outputs, answers.view(-1), ignore_index=pad, reduction='sum') +\\\n",
    "                mutli_target_nll_loss(supporting, relevance)\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if (epoch+1)%10==0:\n",
    "            test_accuracy.append(eval(model, torch.Tensor(test[0]).long(), torch.Tensor(test[1]).long(), \n",
    "      torch.Tensor(test[3]).long(), torch.Tensor(np.argmax(test[2], axis=1)).long()))\n",
    "#             print(f\"Epoch {epoch+1}: {loss.item()/story.shape[0]}\")\n",
    "    return max(test_accuracy)\n",
    "     \n",
    "def eval(model, story, query, supporting, answers):\n",
    "    with torch.no_grad():\n",
    "        _, relevance = model(story, query, supporting)\n",
    "        relevance = np.exp(relevance)\n",
    "        s = np.where(relevance>0.5, 1, 0)\n",
    "        for i, val in enumerate(s):\n",
    "            if sum(val)==0:\n",
    "                third_val = sorted(relevance[i,:], reverse=True)[1]\n",
    "                s[i,:] = np.where(relevance[i,:]>third_val, 1, 0)\n",
    "        \n",
    "        binary_s = torch.Tensor(s).long()\n",
    "        outputs, _ = model(story, query, binary_s)\n",
    "        \n",
    "        predicted_ans = (np.argmax(outputs.detach().numpy(), axis=1))\n",
    "\n",
    "        return (np.mean(predicted_ans == answers.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 1: 24.4%\n",
      "Task 2: 44.7%\n",
      "Task 3: 44.8%\n",
      "Task 4: 27.5%\n",
      "Task 5: 47.1%\n",
      "Task 6: 51.7%\n",
      "Task 7: 53.8%\n",
      "Task 8: 38.9%\n",
      "Task 9: 63.4%\n",
      "Task 10: 82.2%\n",
      "Task 11: 20.1%\n",
      "Task 12: 16.5%\n",
      "Task 13: 33.7%\n",
      "Task 14: 21.9%\n",
      "Task 15: 34.12%\n",
      "Task 16: 36.0%\n",
      "Task 17: 53.44%\n",
      "Task 18: 88.95%\n",
      "Task 19: 10.5%\n",
      "Task 20: 75.42%\n"
     ]
    }
   ],
   "source": [
    "for task in range(1,21):\n",
    "    train, test, vocab, max_story_count, max_story_len, query_maxlen, word_idx = preprocess_data(task)\n",
    "\n",
    "    mem = LSTM_with_attention(len(vocab), 10, 5, 4, 2, max_story_count, query_maxlen, max_story_len)\n",
    "    optimizer = torch.optim.Adam(mem.parameters(), 0.1)\n",
    "    test_accuracy = train_model(mem, optimizer, 100, torch.Tensor(train[0]).long(), torch.Tensor(train[1]).long(), \n",
    "          torch.Tensor(train[3]).long(), torch.Tensor(np.argmax(train[2], axis=1)).long(), test)\n",
    "    print(f\"Task {task}: {np.round(test_accuracy*100,2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
