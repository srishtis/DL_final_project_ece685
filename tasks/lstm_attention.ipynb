{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from functools import reduce \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_files = os.listdir('./')\n",
    "text_files = [i for i in text_files if '.txt' in i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure of all tasks have single word answers\n",
    "for f in text_files:\n",
    "    text1 = pd.read_csv('./'+f, sep=\"\\n\", header=None)\n",
    "    text1.columns = ['text']\n",
    "    ans = []\n",
    "    for t in text1.text:\n",
    "        if '?' in t:\n",
    "            match = re.search(r'[a-zA-z0-9?\\ ]*\\t([\\w \\ ]+)', t)\n",
    "            if match:\n",
    "                ans.append(match.group(1)) \n",
    "                \n",
    "    ans = [i.split(' ') for i in ans]\n",
    "    for i in ans:\n",
    "        if len(i)>1:\n",
    "            print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def tokenize(sent):\n",
    "    return tokenizer.tokenize(sent)\n",
    "\n",
    "def parse_stories(lines):\n",
    "    data = []\n",
    "    story = []\n",
    "    nid_arr = []\n",
    "    for line in lines:\n",
    "        #line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            # reset story when line ID=1 (start of new story)\n",
    "            story = []\n",
    "            nid_arr = []\n",
    "        if '\\t' in line:\n",
    "            # this line is tab separated Q, A &amp;amp;amp;amp;amp; support fact ID\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            # tokenize the words of question\n",
    "            q = tokenize(q)\n",
    "            # Provide all the sub-stories till this question\n",
    "            substory = [x for x in story if x]\n",
    "            # A story ends and is appended to global story data-set\n",
    "            supporting = supporting.split()\n",
    "            supporting = [int(i) for i in supporting]\n",
    "            s = [i for i in range(len(nid_arr)) if nid_arr[i] in supporting]\n",
    "            data.append((substory, q, a, s))\n",
    "            story.append('')\n",
    "        else:\n",
    "            # this line is a sentence of story\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "            nid_arr.append(nid)\n",
    "    return data\n",
    "\n",
    "def get_stories(f):\n",
    "    # read the data file and parse 10k stories\n",
    "    data = parse_stories(f.readlines())\n",
    "    # lambda func to flatten the list of sentences into one list\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    # creating list of tuples for each story\n",
    "    data = [((story), q, answer, s) for story, q, answer, s in data]\n",
    "    #data = [((story), q, answer) for story, q, answer in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./task_2.txt') as f:\n",
    "    all_stories = get_stories(f)\n",
    "    \n",
    "train_stories, test_stories = train_test_split(all_stories, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['Daniel', 'and', 'Sandra', 'went', 'to', 'the', 'office'],\n",
       "  ['Sandra', 'and', 'Daniel', 'moved', 'to', 'the', 'bathroom'],\n",
       "  ['Mary', 'and', 'John', 'travelled', 'to', 'the', 'hallway'],\n",
       "  ['Daniel', 'and', 'John', 'journeyed', 'to', 'the', 'office']],\n",
       " ['Where', 'is', 'John'],\n",
       " 'office',\n",
       " [3])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stories[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_story_count = max([len(story[0]) for story in all_stories])\n",
    "max_story_len = max([max([len(i) for i in story[0]]) for story in all_stories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_query(sequences, query_maxlen):\n",
    "    \"\"\"\n",
    "    :param sequences: list of tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(sequences)\n",
    "    out_tensor = np.zeros((num, query_maxlen))\n",
    "    for i, tensor in enumerate(sequences):\n",
    "        length = len(tensor)\n",
    "        out_tensor[i, :length] = tensor\n",
    "    return out_tensor\n",
    "\n",
    "def padding_story(sequences, max_story_count, max_story_len):\n",
    "    \"\"\"\n",
    "    :param sequences: list of tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(sequences)\n",
    "    out_tensor = np.zeros((num, max_story_count, max_story_len))\n",
    "    for i, stories in enumerate(sequences):\n",
    "        for j, story in enumerate(stories):\n",
    "            length = len(story)\n",
    "            out_tensor[i, j, :length] = story\n",
    "    return out_tensor\n",
    "\n",
    "def padding_supporting(sequences, max_story_count):\n",
    "    \"\"\"\n",
    "    :param sequences: list of tensors\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    num = len(sequences)\n",
    "    out_tensor = np.zeros((num, max_story_count))\n",
    "    for i, s in enumerate(sequences):\n",
    "        for j, t in enumerate(s):\n",
    "            if t>0:\n",
    "                out_tensor[i,t] = 1\n",
    "    return out_tensor\n",
    "\n",
    "def vectorize_stories(data, word_idx, query_maxlen, max_story_count, max_story_len):\n",
    "    # story vector initialization\n",
    "    X = []\n",
    "    # query vector initialization\n",
    "    Xq = []\n",
    "    # answer vector intialization\n",
    "    Y = []\n",
    "    S = []\n",
    "            \n",
    "    for story, query, answer, supporting in data:\n",
    "        # creating list of story word indices\n",
    "        x = []\n",
    "        for sen in story: \n",
    "            s = [word_idx[w] for w in sen]\n",
    "            x.append(s)\n",
    "        # creating list of query word indices\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx))\n",
    "        # creating label 1 for the answer word index\n",
    "        y[word_idx[answer]] = 1\n",
    "        X.append(x)\n",
    "        Xq.append(xq)\n",
    "        Y.append(y)\n",
    "        S.append(supporting)\n",
    "        \n",
    "    return (padding_story(X, max_story_count, max_story_len),\n",
    "            padding_query(Xq, query_maxlen), np.array(Y), padding_supporting(S, max_story_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "vocab = set()\n",
    "for story, q, answer, s in train_stories + test_stories:\n",
    "    vocab |= set(flatten(story) + q + [answer])\n",
    "    \n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab) + 1\n",
    "#story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n",
    "vocab = list(vocab)\n",
    "query_maxlen = max(map(len, (x for _, x, _,_ in train_stories + test_stories)))\n",
    "vocab = ['<pad>'] + vocab\n",
    "word_idx = dict((c, i) for i, c in enumerate(vocab))\n",
    "idx_word = dict((i, c) for i,c in enumerate(vocab))\n",
    "\n",
    "inputs_train, queries_train, answers_train, supporting_train = vectorize_stories(train_stories,\n",
    "                                                               word_idx,\n",
    "                                                               query_maxlen, max_story_count, max_story_len)\n",
    "inputs_test, queries_test, answers_test, supporting_test = vectorize_stories(test_stories,\n",
    "                                                               word_idx,\n",
    "                                                               query_maxlen, max_story_count, max_story_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_with_attention(nn.Module):\n",
    "    def __init__(self, vocab_size, story_embed_size, query_embed_size, story_hidden_dim, query_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.story_embed_size = story_embed_size \n",
    "        self.story_hidden_dim = story_hidden_dim\n",
    "        self.query_hidden_dim = query_hidden_dim\n",
    "        self.story_embeddings = nn.Embedding(vocab_size, story_embed_size)\n",
    "        self.query_embeddings = nn.Embedding(vocab_size, query_embed_size)\n",
    "        self.story_lstm = nn.LSTM(story_embed_size*6, story_hidden_dim)\n",
    "        self.query_lstm = nn.LSTM(query_embed_size, query_hidden_dim)\n",
    "        self.story_hidden = self.init_story_hidden(max_story_count)  \n",
    "        self.query_hidden = self.init_query_hidden(query_maxlen) \n",
    "        self.hidden2label = nn.Linear(story_hidden_dim*max_story_count+query_hidden_dim*query_maxlen, vocab_size)\n",
    "        self.story_relevance = nn.Linear(story_hidden_dim*max_story_count, max_story_count)\n",
    "        \n",
    "    def forward(self, input_sequence, query, supported):\n",
    "        supporting_stories = input_sequence*supported.unsqueeze(2)\n",
    "        supporting_story_embed = self.story_embeddings(supporting_stories)\n",
    "        all_supporting_story_embed = self.story_embeddings(input_sequence)\n",
    "        query_embedding = self.query_embeddings(query)\n",
    "        supporting_story_embed = supporting_story_embed.view(supporting_story_embed.shape[0], supporting_story_embed.shape[1], -1)\n",
    "        all_supporting_story_embed = all_supporting_story_embed.view(all_supporting_story_embed.shape[0], all_supporting_story_embed.shape[1], -1)\n",
    "        lstm_story_out, _ = self.story_lstm(supporting_story_embed, self.story_hidden)\n",
    "        lstm_story_out_all, _ = self.story_lstm(all_supporting_story_embed, self.story_hidden)\n",
    "        lstm_query_out, _ = self.query_lstm(query_embedding, self.query_hidden)\n",
    "        s = lstm_story_out.view(len(lstm_story_out),-1)\n",
    "        s_all = lstm_story_out_all.view(len(lstm_story_out_all),-1)\n",
    "        q = lstm_query_out.view(len(lstm_query_out),-1)\n",
    "        \n",
    "        c = torch.cat([s,q], dim=1)\n",
    "        y  = self.hidden2label(c)\n",
    "        r = self.story_relevance(s_all)\n",
    "        log_probs = F.log_softmax(y)\n",
    "        log_probs_r = F.log_softmax(r)\n",
    "        return log_probs, log_probs_r\n",
    "        \n",
    "    \n",
    "        \n",
    "    def init_story_hidden(self, max_len):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, max_len, self.story_hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, max_len, self.story_hidden_dim)))\n",
    "    \n",
    "    def init_query_hidden(self, max_len):\n",
    "        # the first is the hidden h\n",
    "        # the second is the cell  c\n",
    "        return (autograd.Variable(torch.zeros(1, max_len, self.query_hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, max_len, self.query_hidden_dim)))\n",
    "\n",
    "def mutli_target_nll_loss(actual, predicted):\n",
    "    m = (actual*predicted).sum(axis=1)\n",
    "    loss = m.sum()\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "mem = LSTM_with_attention(len(vocab), 10, 2, 5, 2)\n",
    "x=mem(torch.Tensor(inputs_train).long(), torch.Tensor(queries_train).long(), torch.Tensor(supporting_train).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(False)\n",
    "def train(model, optimizer, epochs, story, query, supporting, answers):\n",
    "    pad = word_idx['<pad>']\n",
    "    for epoch in range(epochs):\n",
    "        #scheduler.step()\n",
    "        epoch_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        outputs, relevance = model(story, query, supporting)\n",
    "        loss = F.nll_loss(outputs, answers.view(-1), ignore_index=pad, reduction='sum') +\\\n",
    "                mutli_target_nll_loss(supporting, relevance)\n",
    "#         print(F.nll_loss(outputs, answers.view(-1), ignore_index=pad, reduction='sum'))\n",
    "#         print(mutli_target_nll_loss(supporting, relevance))\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if epoch%5==0:\n",
    "            eval(model, story, query, supporting, answers)\n",
    "        print(f\"Epoch {epoch+1}: {loss.item()/story.shape[0]}\")\n",
    "     \n",
    "def eval(model, story, query, supporting, answers):\n",
    "    with torch.no_grad():\n",
    "        _, relevance = model(story, query, supporting)\n",
    "        relevance = np.exp(relevance)\n",
    "        s = np.where(relevance>0.5, 1, 0)\n",
    "        for i, val in enumerate(s):\n",
    "            if sum(val)==0:\n",
    "                third_val = sorted(relevance[i,:], reverse=True)[3]\n",
    "                s[i,:] = np.where(relevance[i,:]>third_val, 1, 0)\n",
    "        \n",
    "        binary_s = torch.Tensor(s).long()\n",
    "        outputs, _ = model(story, query, binary_s)\n",
    "        \n",
    "#         ans = np.argmax(answers, axis=1)\n",
    "        predicted_ans = (np.argmax(outputs.detach().numpy(), axis=1))\n",
    "#         print(predicted_ans)\n",
    "#         print(answers)\n",
    "        print(f\"test accuracy: {np.mean(predicted_ans == answers.numpy())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:34: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.10925\n",
      "Epoch 1: 12.120236328125\n",
      "Epoch 2: 13.6616494140625\n",
      "Epoch 3: 21.87064453125\n",
      "Epoch 4: 12.9696728515625\n",
      "Epoch 5: 14.248076171875\n",
      "test accuracy: 0.29075\n",
      "Epoch 6: 10.3891533203125\n",
      "Epoch 7: 9.7696201171875\n",
      "Epoch 8: 8.95551953125\n",
      "Epoch 9: 8.9431220703125\n",
      "Epoch 10: 9.15401171875\n",
      "test accuracy: 0.2715\n",
      "Epoch 11: 8.95028125\n",
      "Epoch 12: 8.74775390625\n",
      "Epoch 13: 8.8492236328125\n",
      "Epoch 14: 8.752359375\n",
      "Epoch 15: 8.5243388671875\n",
      "test accuracy: 0.29775\n",
      "Epoch 16: 8.4611298828125\n",
      "Epoch 17: 8.42096875\n",
      "Epoch 18: 8.28250390625\n",
      "Epoch 19: 8.08012158203125\n",
      "Epoch 20: 7.91369970703125\n",
      "test accuracy: 0.384\n",
      "Epoch 21: 7.8010400390625\n",
      "Epoch 22: 7.643806640625\n",
      "Epoch 23: 7.47537255859375\n",
      "Epoch 24: 7.38053857421875\n",
      "Epoch 25: 7.27842333984375\n",
      "test accuracy: 0.409\n",
      "Epoch 26: 7.11930908203125\n",
      "Epoch 27: 6.9864765625\n",
      "Epoch 28: 6.92313818359375\n",
      "Epoch 29: 6.822244140625\n",
      "Epoch 30: 6.7232275390625\n",
      "test accuracy: 0.3955\n",
      "Epoch 31: 6.681439453125\n",
      "Epoch 32: 6.6542607421875\n",
      "Epoch 33: 6.61216943359375\n",
      "Epoch 34: 6.5752060546875\n",
      "Epoch 35: 6.546888671875\n",
      "test accuracy: 0.3975\n",
      "Epoch 36: 6.49006787109375\n",
      "Epoch 37: 6.4094423828125\n",
      "Epoch 38: 6.3472763671875\n",
      "Epoch 39: 6.3016533203125\n",
      "Epoch 40: 6.25964892578125\n",
      "test accuracy: 0.38825\n",
      "Epoch 41: 6.2228994140625\n",
      "Epoch 42: 6.1879482421875\n",
      "Epoch 43: 6.14517138671875\n",
      "Epoch 44: 6.10419140625\n",
      "Epoch 45: 6.07491162109375\n",
      "test accuracy: 0.40425\n",
      "Epoch 46: 6.04376318359375\n",
      "Epoch 47: 6.0117607421875\n",
      "Epoch 48: 5.98103564453125\n",
      "Epoch 49: 5.9437119140625\n",
      "Epoch 50: 5.9112373046875\n",
      "test accuracy: 0.41225\n",
      "Epoch 51: 5.8844072265625\n",
      "Epoch 52: 5.85577294921875\n",
      "Epoch 53: 5.83217236328125\n",
      "Epoch 54: 5.80825732421875\n",
      "Epoch 55: 5.7825068359375\n",
      "test accuracy: 0.41425\n",
      "Epoch 56: 5.7602734375\n",
      "Epoch 57: 5.7351630859375\n",
      "Epoch 58: 5.71225439453125\n",
      "Epoch 59: 5.689775390625\n",
      "Epoch 60: 5.6664609375\n",
      "test accuracy: 0.407\n",
      "Epoch 61: 5.6449326171875\n",
      "Epoch 62: 5.621865234375\n",
      "Epoch 63: 5.60120654296875\n",
      "Epoch 64: 5.581001953125\n",
      "Epoch 65: 5.5617568359375\n",
      "test accuracy: 0.41875\n",
      "Epoch 66: 5.542361328125\n",
      "Epoch 67: 5.522017578125\n",
      "Epoch 68: 5.50335791015625\n",
      "Epoch 69: 5.48513525390625\n",
      "Epoch 70: 5.46771630859375\n",
      "test accuracy: 0.4135\n",
      "Epoch 71: 5.44978076171875\n",
      "Epoch 72: 5.4330400390625\n",
      "Epoch 73: 5.4164716796875\n",
      "Epoch 74: 5.39964892578125\n",
      "Epoch 75: 5.3842265625\n",
      "test accuracy: 0.4085\n",
      "Epoch 76: 5.367638671875\n",
      "Epoch 77: 5.35303955078125\n",
      "Epoch 78: 5.33754296875\n",
      "Epoch 79: 5.323193359375\n",
      "Epoch 80: 5.308302734375\n"
     ]
    }
   ],
   "source": [
    "mem = LSTM_with_attention(len(vocab), 10, 2, 5, 2)\n",
    "optimizer = torch.optim.Adam(mem.parameters(), 0.1)\n",
    "train(mem, optimizer, 80, torch.Tensor(inputs_train).long(), torch.Tensor(queries_train).long(), \n",
    "      torch.Tensor(supporting_train).long(), torch.Tensor(np.argmax(answers_train, axis=1)).long())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
