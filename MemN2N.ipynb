{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import BABI20\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch.nn.init as I\n",
    "import numpy as np\n",
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(batch_size, memory_size, task, joint, tenK):\n",
    "    train_iter, valid_iter, test_iter = BABI20.iters(\n",
    "        batch_size=batch_size, memory_size=memory_size, task=task, joint=joint, tenK=tenK, device=torch.device(\"cpu\"),\n",
    "    shuffle=True)\n",
    "    return train_iter, valid_iter, test_iter, train_iter.dataset.fields['query'].vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter, vocab = dataloader(64, 50, 6, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_story(stories):\n",
    "    \"\"\"\n",
    "    function to print stories from padded sequence\n",
    "    \"\"\"\n",
    "    for s in stories:\n",
    "        if sum(s)>0:\n",
    "            print(' '.join([vocab.itos[i] for i in s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The chest fits inside the suitcase <pad> <pad> <pad> <pad> <pad>\n",
      "The box of chocolates fits inside the box <pad> <pad> <pad>\n",
      "The container is bigger than the box <pad> <pad> <pad> <pad>\n",
      "The box fits inside the suitcase <pad> <pad> <pad> <pad> <pad>\n",
      "The suitcase is bigger than the chest <pad> <pad> <pad> <pad>\n",
      "['Is', 'the', 'box', 'of', 'chocolates', 'bigger', 'than', 'the', 'suitcase', '<pad>', '<pad>']\n",
      "no\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for _, batch in enumerate(train_iter, start=1):\n",
    "    print_story(batch.story[0])\n",
    "    print([vocab.itos[i] for i in batch.query[0]])\n",
    "    print(vocab.itos[batch.answer[0]])\n",
    "    break\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemN2N(nn.Module):\n",
    "\n",
    "    def __init__(self, params, vocab):\n",
    "        super(MemN2N, self).__init__()\n",
    "        self.input_size = len(vocab)\n",
    "        self.embed_size = params['embed_size']\n",
    "        self.memory_size = params['memory_size']\n",
    "        self.num_hops = params['num_hops']\n",
    "        self.use_bow = params['use_bow']\n",
    "        self.use_lw = params['use_lw']\n",
    "        self.use_ls = params['use_ls']\n",
    "        self.vocab = vocab\n",
    "\n",
    "        # create parameters according to different type of weight tying\n",
    "        pad = self.vocab.stoi['<pad>']\n",
    "        # First embedding for stories\n",
    "        self.A = nn.ModuleList([nn.Embedding(self.input_size, self.embed_size, padding_idx=pad)])\n",
    "        self.A[-1].weight.data.normal_(0, 0.1)\n",
    "        # Second embedding for stories\n",
    "        self.C = nn.ModuleList([nn.Embedding(self.input_size, self.embed_size, padding_idx=pad)])\n",
    "        self.C[-1].weight.data.normal_(0, 0.1)\n",
    "        if self.use_lw:\n",
    "            for _ in range(1, self.num_hops):\n",
    "                self.A.append(self.A[-1])\n",
    "                self.C.append(self.C[-1])\n",
    "            self.B = nn.Embedding(self.input_size, self.embed_size, padding_idx=pad)\n",
    "            self.B.weight.data.normal_(0, 0.1)\n",
    "            self.out = nn.Parameter(\n",
    "                I.normal_(torch.empty(self.input_size, self.embed_size), 0, 0.1))\n",
    "            self.H = nn.Linear(self.embed_size, self.embed_size)\n",
    "            self.H.weight.data.normal_(0, 0.1)\n",
    "        else:\n",
    "            for _ in range(1, self.num_hops):\n",
    "                self.A.append(self.C[-1])\n",
    "                self.C.append(nn.Embedding(self.input_size, self.embed_size, padding_idx=pad))\n",
    "                self.C[-1].weight.data.normal_(0, 0.1)\n",
    "            self.B = self.A[0]\n",
    "            self.out = self.C[-1].weight\n",
    "\n",
    "        # temporal matrix\n",
    "        self.TA = nn.Parameter(I.normal_(torch.empty(self.memory_size, self.embed_size), 0, 0.1))\n",
    "        self.TC = nn.Parameter(I.normal_(torch.empty(self.memory_size, self.embed_size), 0, 0.1))\n",
    "\n",
    "    def forward(self, story, query):\n",
    "        sen_size = query.shape[-1]\n",
    "        weights = self.compute_weights(sen_size)\n",
    "        state = (self.B(query) * weights).sum(1)\n",
    "        sen_size = story.shape[-1]\n",
    "        weights = self.compute_weights(sen_size)\n",
    "        for i in range(self.num_hops):\n",
    "            memory = (self.A[i](story.view(-1, sen_size)) * weights).sum(1).view(\n",
    "                *story.shape[:-1], -1)\n",
    "            memory += self.TA\n",
    "            output = (self.C[i](story.view(-1, sen_size)) * weights).sum(1).view(\n",
    "                *story.shape[:-1], -1)\n",
    "            output += self.TC\n",
    "            probs = (memory @ state.unsqueeze(-1)).squeeze() # attention scores\n",
    "            if not self.use_ls:\n",
    "                probs = F.softmax(probs, dim=-1)\n",
    "            response = (probs.unsqueeze(1) @ output).squeeze()\n",
    "            if self.use_lw:\n",
    "                state = self.H(response) + state\n",
    "            else:\n",
    "                state = response + state\n",
    "\n",
    "        return F.log_softmax(F.linear(state, self.out), dim=-1)\n",
    "\n",
    "    def compute_weights(self, J):\n",
    "        # position encoding\n",
    "        d = self.embed_size\n",
    "        if self.use_bow:\n",
    "            weights = torch.ones(J, d)\n",
    "        else:\n",
    "            func = lambda j, k: 1 - (j + 1) / J - (k + 1) / d * (1 - 2 * (j + 1) / J)    # 0-based indexing\n",
    "            weights = torch.from_numpy(np.fromfunction(func, (J, d), dtype=np.float32))\n",
    "        #return weights.cuda() if torch.cuda.is_available() else weights\n",
    "        return weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, model, optimizer, epochs, max_clip, valid_iter=None):\n",
    "    total_loss = 0\n",
    "    valid_data = list(valid_iter)\n",
    "    valid_loss = None\n",
    "    next_epoch_to_report = 5\n",
    "    pad = model.vocab.stoi['<pad>']\n",
    "\n",
    "    for _, batch in enumerate(train_iter, start=1):\n",
    "        story = batch.story\n",
    "        query = batch.query\n",
    "        answer = batch.answer\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(story, query)\n",
    "        loss = F.nll_loss(outputs, answer.view(-1), ignore_index=pad, reduction='sum')\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        # linear start\n",
    "        if model.use_ls:\n",
    "            loss = 0\n",
    "            for k, batch in enumerate(valid_data, start=1):\n",
    "                story = batch.story\n",
    "                query = batch.query\n",
    "                answer = batch.answer\n",
    "                outputs = model(story, query)\n",
    "                loss += F.nll_loss(outputs, answer.view(-1), ignore_index=pad, reduction='sum').item()\n",
    "            loss = loss / k\n",
    "            if valid_loss and valid_loss <= loss:\n",
    "                model.use_ls = False\n",
    "            else:\n",
    "                valid_loss = loss\n",
    "\n",
    "        if train_iter.epoch == next_epoch_to_report:\n",
    "            print(\"#! epoch {:d} average batch loss: {:5.4f}\".format(\n",
    "                int(train_iter.epoch), total_loss / len(train_iter)))\n",
    "            next_epoch_to_report += 5\n",
    "        if int(train_iter.epoch) == train_iter.epoch:\n",
    "            total_loss = 0\n",
    "        if train_iter.epoch == epochs:\n",
    "            print(\"Done!\")\n",
    "            break\n",
    "            \n",
    "def eval(test_iter, model):\n",
    "    total_error = 0\n",
    "\n",
    "    for k, batch in enumerate(test_iter, start=1):\n",
    "        story = batch.story\n",
    "        query = batch.query\n",
    "        answer = batch.answer\n",
    "        outputs = model(story, query)\n",
    "        _, outputs = torch.max(outputs, -1)\n",
    "        total_error += torch.mean((outputs != answer.view(-1)).float()).item()\n",
    "    print(\"#! average error: {:5.1f}\".format(total_error / k * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter, model, optimizer, epochs, max_clip, valid_iter=None):\n",
    "    total_loss = 0\n",
    "    next_epoch_to_report = 5\n",
    "    pad = model.vocab.stoi['<pad>']\n",
    "    scheduler = StepLR(optimizer, step_size=25, gamma=0.5)\n",
    "    for epoch in range(epochs):\n",
    "        scheduler.step()\n",
    "        epoch_loss = 0\n",
    "        training_data_size = 0\n",
    "        for _, batch in enumerate(train_iter, start=1):\n",
    "            training_data_size += batch.answer.shape[0]\n",
    "            story = batch.story\n",
    "            query = batch.query\n",
    "            answer = batch.answer\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(story, query)\n",
    "            loss = F.nll_loss(outputs, answer.view(-1), ignore_index=pad, reduction='sum')\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_clip)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss for epoch 0: 4.265222820070055\n",
      "Loss for epoch 1: 0.6998750078413222\n",
      "Loss for epoch 2: 0.6788641730414496\n",
      "Loss for epoch 3: 0.46520428297254773\n",
      "Loss for epoch 4: 0.39745666207207575\n",
      "Loss for epoch 5: 0.372959529876709\n",
      "Loss for epoch 6: 0.36126250563727486\n",
      "Loss for epoch 7: 0.35352439075046116\n",
      "Loss for epoch 8: 0.35828187582227916\n",
      "Loss for epoch 9: 0.35167537763383655\n",
      "Loss for epoch 10: 0.34342841127183704\n",
      "Loss for epoch 11: 0.3414072438346015\n",
      "Loss for epoch 12: 0.34585595565372046\n",
      "Loss for epoch 13: 0.3344229125976563\n",
      "Loss for epoch 14: 0.3314963086446126\n",
      "Loss for epoch 15: 0.3279499698215061\n",
      "Loss for epoch 16: 0.33579450289408364\n",
      "Loss for epoch 17: 0.3312346231672499\n",
      "Loss for epoch 18: 0.33089186032613116\n",
      "Loss for epoch 19: 0.33001532713572185\n"
     ]
    }
   ],
   "source": [
    "params = {'embed_size' : 20,\n",
    "          'memory_size' : 50,\n",
    "          'num_hops' : 3,\n",
    "          'use_bow' : False,\n",
    "          'use_lw' : True,\n",
    "          'use_ls' : True}\n",
    "\n",
    "model = MemN2N(params, vocab)\n",
    "optimizer = optim.Adam(model.parameters(), 0.01)\n",
    "train(train_iter, model, optimizer, 20, 40, valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daniel travelled to the kitchen <pad>\n",
      "Daniel moved to the bedroom <pad>\n",
      "Mary went to the garden <pad>\n",
      "John took the football there <pad>\n",
      "Is Daniel in the bathroom\n",
      "no\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(test_iter, start=1):\n",
    "    vocab = test_iter.dataset.fields['query'].vocab\n",
    "    with torch.no_grad():\n",
    "        story = batch.story\n",
    "        query = batch.query\n",
    "        answer = batch.answer\n",
    "        outputs = model(story, query)\n",
    "        ex = 33\n",
    "        print_story(story[ex])\n",
    "        print(' '.join([vocab.itos[i] for i in query[ex]]))\n",
    "        print(vocab.itos[answer[ex]])\n",
    "        o = np.argmax(outputs[ex])\n",
    "        print(vocab.itos[o])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = []\n",
    "for _, batch in enumerate(test_iter, start=1):\n",
    "    vocab = test_iter.dataset.fields['query'].vocab\n",
    "    with torch.no_grad():\n",
    "        story = batch.story\n",
    "        query = batch.query\n",
    "        answer = batch.answer\n",
    "        outputs = model(story, query)\n",
    "        o = np.argmax(outputs, axis=1)\n",
    "        acc += (list(np.array(answer.reshape(-1,)==o)))\n",
    "np.mean(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy for Task 1: 100.0%\n",
      "Test Accuracy for Task 2: 14.499999999999998%\n",
      "Test Accuracy for Task 3: 16.6%\n",
      "Test Accuracy for Task 4: 61.5%\n",
      "Test Accuracy for Task 5: 78.7%\n",
      "Test Accuracy for Task 6: 49.3%\n",
      "Test Accuracy for Task 7: 47.699999999999996%\n",
      "Test Accuracy for Task 8: 85.8%\n",
      "Test Accuracy for Task 9: 63.800000000000004%\n",
      "Test Accuracy for Task 10: 44.3%\n",
      "Test Accuracy for Task 11: 33.1%\n",
      "Test Accuracy for Task 12: 77.2%\n",
      "Test Accuracy for Task 13: 94.39999999999999%\n",
      "Test Accuracy for Task 14: 20.1%\n",
      "Test Accuracy for Task 15: 22.1%\n",
      "Test Accuracy for Task 16: 25.6%\n",
      "Test Accuracy for Task 17: 54.1%\n",
      "Test Accuracy for Task 18: 65.4%\n",
      "Test Accuracy for Task 19: 8.1%\n",
      "Test Accuracy for Task 20: 90.9%\n"
     ]
    }
   ],
   "source": [
    "params = {'embed_size' : 30,\n",
    "          'memory_size' : 50,\n",
    "          'num_hops' : 4,\n",
    "          'use_bow' : False,\n",
    "          'use_lw' : True,\n",
    "          'use_ls' : False}\n",
    "\n",
    "for task in range(1,21):\n",
    "    train_iter, valid_iter, test_iter, vocab = dataloader(64, 50, task, False, True)\n",
    "    model = MemN2N(params, vocab)\n",
    "    optimizer = optim.Adam(model.parameters(), 0.05)\n",
    "    train(train_iter, model, optimizer, 20, 40, valid_iter)\n",
    "    acc = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch in enumerate(test_iter, start=1):\n",
    "            with torch.no_grad():\n",
    "                story = batch.story\n",
    "                query = batch.query\n",
    "                answer = batch.answer\n",
    "                outputs = model(story, query)\n",
    "                o = np.argmax(outputs, axis=1)\n",
    "                acc += (list(np.array(answer.reshape(-1,)==o)))\n",
    "        print(f\"Test Accuracy for Task {task}: {np.mean(acc)*100}%\")\n",
    "    np.mean(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
